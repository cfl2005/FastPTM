# FastPTM : Fast Weight Loading for Inference Acceleration of Pre-Trained Models in GPUs
Large-scale pre-trained models (PTMs) have achieved great success on a wide range of NLP and CV tasks and have become milestones in the field of deep learning (DL). Despite the effectiveness of PTMs, PTMs are typically associated with large memory and high computational requirements, which increase the cost and time of inference. To enable large-scale deployment and real-time response of PTM applications, we propose the FastPTM framework, a more general framework for accelerating the inference tasks of PTMs, deployed in edge computing. In the framework, we implement a fast weight loading method based on weight and model separation to efficiently accelerate inference tasks for large-size models in resource-constrained edge environments. In addition, we design an online scheduling algorithm to reduce the task loading time and inference time. Experiment results show that FastPTM can improve 8.2 times of processing speed and significantly reduce the timeout risk of user requests and improve the real-time and timeliness of response.
